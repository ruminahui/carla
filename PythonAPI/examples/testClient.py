#!/usr/bin/env python

# Copyright (c) 2019 Computer Vision Center (CVC) at the Universitat Autonoma de
# Barcelona (UAB).
#
# This work is licensed under the terms of the MIT license.
# For a copy, see <https://opensource.org/licenses/MIT>.

import glob
import os
import sys

# try:
#     sys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (
#         sys.version_info.major,
#         sys.version_info.minor,
#         'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])
# except IndexError:
#     pass

import carla

import random
import time
import numpy as np
# Agent trained from imitation learning
import image_converter
from carla import ColorConverter as cc
from learning_agents.imitation.imitation_agent import ImitationAgent
from measurements import Measurements


REACH_GOAL = 0.0
GO_STRAIGHT = 5.0
TURN_RIGHT = 4.0
TURN_LEFT = 3.0
LANE_FOLLOW = 2.0


def pack_data(image, player, agent):
    """ Function to pack up the data into the necessary format for the agent to process """
    meas =  Measurements()
    meas.player_measurements.forward_speed = player.get_forward_speed() * float(0.036)
    sensor_data = {}
    # Do some processing of the image data
    image.convert(cc.Raw)
    array = np.frombuffer(image.raw_data, dtype=np.dtype("uint8"))
    array = np.reshape(array, (image.height, image.width, 4))
    array = array[:, :, :3]
    array = array[:, :, ::-1]

    # Store processed data to pass to agent
    sensor_data['CameraRGB'] = array

    # Process next control step from the agent and execute it
    control = agent.run_step(meas, sensor_data, GO_STRAIGHT, None)

    player.apply_control(control)



def main():
    actor_list = []

    # In this tutorial script, we are going to add a vehicle to the simulation
    # and let it drive in autopilot. We will also create a camera attached to
    # that vehicle, and save all the images generated by the camera to disk.

    try:
        # First of all, we need to create the client that will send the requests
        # to the simulator. Here we'll assume the simulator is accepting
        # requests in the localhost at port 2000.
        client = carla.Client('localhost', 2000)
        client.set_timeout(2.0)

        # Once we have a client we can retrieve the world that is currently
        # running.
        world = client.get_world()

        # Get a spectator
        spectator = world.get_spectator()

        # The world contains the list blueprints that we can use for adding new
        # actors into the simulation.
        blueprint_library = world.get_blueprint_library()

        # Now let's filter all the blueprints of type 'vehicle' and choose one
        # at random.
        bp = random.choice(blueprint_library.filter('vehicle'))

        # Find the car actor in the world

        car_actor = None
        for actor in world.get_actors():
            if("vehicle" in actor.type_id):
                car_actor = actor
                break

        if(car_actor == None):
            raise NoActorError("No actor found")

        # Now have to retrieve the transform of the car in the simulation world 
        # and attach an spectator relative to this transform
        world.tick()

        world_snapshot = world.wait_for_tick()
        actor_snapshot = world_snapshot.find(car_actor.id)
        spectator.set_transform(actor_snapshot.get_transform())

        # Test out the forward speed method
        print("Forward speed computed is")
        print(car_actor.get_forward_speed())


        # Initiate an agent to control the car
        agent = ImitationAgent('Town01', True, car_actor)

        ## Attach a camera to the vehicle and feed in sensory values to the controller of the car
        camera_bp = blueprint_library.find('sensor.camera.rgb')
        Attachment = carla.AttachmentType
        camera_transform = carla.Transform(carla.Location(x=-5.5, z=2.5), carla.Rotation(pitch=8.0))
        camera = world.spawn_actor(camera_bp, camera_transform, attach_to=car_actor, attachment_type=Attachment.SpringArm)
        actor_list.append(camera)
        camera.listen(lambda data: pack_data(data, car_actor, agent))
        print('created %s' % camera.type_id)

        while(True):
            # Sleep while waiting for new sensory information
            time.sleep(5)

    finally:

        print('destroying actors')
        for actor in actor_list:
            actor.destroy()
        print('done.')


if __name__ == '__main__':

    main()
